import os
import sys
import json
import httpx
from typing import List, Dict, Any, Optional
from mcp.server.fastmcp import FastMCP
from pydantic import ValidationError

# Import our models
from .models import (
    ModelConfig, 
    InferenceParameters, 
    SamplerType,
    SamplerColumnConfig,
    CategorySamplerParams,
    UniformSamplerParams,
    BernoulliSamplerParams,
    ExpressionColumnConfig,
    LLMTextColumnConfig,
    LLMStructuredColumnConfig,
    LLMJudgeColumnConfig,
    Score,
    DataDesignerConfig
)

# Initialize FastMCP
mcp = FastMCP("nemo-data-designer-py")

# Configuration
NEMO_API_URL = os.environ.get("NEMO_DATA_DESIGNER_URL", "http://localhost:8080")

@mcp.tool()
def create_model_config(
    alias: str, 
    model: str, 
    provider: str = "nvidiabuild", 
    temperature: float = 0.6,
    top_p: float = 0.95,
    max_tokens: int = 1024
) -> Dict[str, Any]:
    """
    Create a model configuration for the NeMo Data Designer.
    
    Args:
        alias: A descriptive name for referring to this model configuration (e.g. "gpt4-turbo")
        model: The model ID from the provider (e.g. "nvidia/llama-3.1-405b-instruct")
        provider: The model provider service (default: "nvidiabuild")
        temperature: Sampling temperature (0.0 to 1.0)
        top_p: Nucleus sampling parameter
        max_tokens: Maximum tokens to generate
    """
    params = InferenceParameters(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens
    )
    config = ModelConfig(
        alias=alias,
        model=model,
        provider=provider,
        inference_parameters=params
    )
    return config.model_dump()

@mcp.tool()
def create_category_sampler(
    name: str,
    values: List[Any]
) -> Dict[str, Any]:
    """
    Create a sampler column that picks from a list of categories.
    
    Args:
        name: Name of the column
        values: List of possible values to sample from (strings, numbers, etc.)
    """
    params = CategorySamplerParams(values=values)
    config = SamplerColumnConfig(
        name=name,
        sampler_type=SamplerType.CATEGORY,
        params=params
    )
    return config.model_dump()

@mcp.tool()
def create_uniform_sampler(
    name: str,
    low: float,
    high: float
) -> Dict[str, Any]:
    """
    Create a sampler column that picks a number from a uniform distribution.
    
    Args:
        name: Name of the column
        low: Minimum value
        high: Maximum value
    """
    params = UniformSamplerParams(low=low, high=high)
    config = SamplerColumnConfig(
        name=name,
        sampler_type=SamplerType.UNIFORM,
        params=params
    )
    return config.model_dump()

@mcp.tool()
def create_bernoulli_sampler(
    name: str,
    p: float = 0.5
) -> Dict[str, Any]:
    """
    Create a sampler column that returns True/False (1/0) with probability p.
    
    Args:
        name: Name of the column
        p: Probability of success (1.0)
    """
    params = BernoulliSamplerParams(p=p)
    config = SamplerColumnConfig(
        name=name,
        sampler_type=SamplerType.BERNOULLI,
        params=params
    )
    return config.model_dump()

@mcp.tool()
def create_expression_column(
    name: str,
    expression: str,
    dtype: str = "float"
) -> Dict[str, Any]:
    """
    Create a column calculated from other columns using a Jinja2 expression.
    
    Args:
        name: Name of the column
        expression: Jinja2 expression (e.g. "{{ (price * 10) | round(2) }}")
        dtype: Data type of the result ("float", "int", "str", "bool")
    """
    config = ExpressionColumnConfig(
        name=name,
        expr=expression,
        dtype=dtype
    )
    return config.model_dump()

@mcp.tool()
def create_llm_text_column(
    name: str,
    model_alias: str,
    prompt_template: str,
    system_prompt: str = ""
) -> Dict[str, Any]:
    """
    Create a column generated by an LLM returning text.
    
    Args:
        name: Name of the column
        model_alias: potentially referring to a model config created earlier
        prompt_template: Jinja2 template for the user prompt
        system_prompt: Optional system prompt
    """
    config = LLMTextColumnConfig(
        name=name,
        model_alias=model_alias,
        prompt=prompt_template,
        system_prompt=system_prompt
    )
    return config.model_dump()

@mcp.tool()
def create_llm_structured_column(
    name: str,
    model_alias: str,
    prompt_template: str,
    schema: Dict[str, Any],
    system_prompt: str = ""
) -> Dict[str, Any]:
    """
    Create a column generated by an LLM returning structured JSON data.
    
    Args:
        name: Name of the column
        model_alias: Model alias to use
        prompt_template: The prompt template
        schema: JSON Schema object defining the structure (passed as dictionary)
        system_prompt: Optional system prompt
    """
    config = LLMStructuredColumnConfig(
        name=name,
        model_alias=model_alias,
        prompt=prompt_template,
        system_prompt=system_prompt,
        output_format=schema
    )
    return config.model_dump()

@mcp.tool()
def create_score(
    name: str,
    description: str,
    options: Dict[str, str]
) -> Dict[str, Any]:
    """
    Create a scoring rubric for LLM-as-a-Judge.
    
    Args:
        name: Name of the score (e.g. "Accuracy")
        description: Description of what is being evaluated
        options: Dictionary mapping score labels to their definitions
    """
    score = Score(
        name=name,
        description=description,
        options=options
    )
    return score.model_dump()

@mcp.tool()
def create_llm_judge_column(
    name: str,
    model_alias: str,
    prompt_template: str,
    scores: List[Dict[str, Any]],
    system_prompt: str = ""
) -> Dict[str, Any]:
    """
    Create a column that evaluates other columns using an LLM judge.
    
    Args:
        name: Name of the column
        model_alias: Model to use for judging
        prompt_template: Prompt template for the judge
        scores: List of Score objects (created via create_score)
        system_prompt: Optional system prompt
    """
    # Validate scores
    validated_scores = [Score(**s) for s in scores]
    
    config = LLMJudgeColumnConfig(
        name=name,
        model_alias=model_alias,
        prompt=prompt_template,
        scores=validated_scores,
        system_prompt=system_prompt
    )
    return config.model_dump()

@mcp.tool()
def submit_job(
    job_name: str,
    model_configs: List[Dict[str, Any]],
    column_configs: List[Dict[str, Any]],
    num_samples: int = 100
) -> str:
    """
    Submit a synthetic data generation job to the NeMo Data Designer service.
    
    Args:
        job_name: Name of the job
        model_configs: List of model configuration objects (created via create_model_config)
        column_configs: List of column configuration objects (created via create_*_column tools)
        num_samples: Number of samples to generate
    """
    
    # In a real implementation, we would construct a payload matching the API expectation.
    # Since we are interfacing with a REST API that expects a certain structure, 
    # we'll map our internal Pydantic models to the API payload.
    # Note: The Notebook uses the Python SDK which constructs these objects. 
    # The API payload structure might slightly differ or the SDK might send a serialized version.
    # For this implementation, we will assume we can send a JSON payload that the API accepts 
    # or that we are simulating the SDK client logic here.
    
    # As the user provided notebook shows SDK usage, the underlying API likely accepts
    # a JSON equivalent of these structures.
    
    payload = {
        "name": job_name,
        "config": {
            "models": model_configs,
            "columns": column_configs,
        },
        "num_samples": num_samples,
        "type": "custom" # Assuming a 'custom' type for this highly configurable flow
    }
    
    try:
        # Using httpx to POST to the service
        with httpx.Client() as client:
            # Note: The endpoint /v1/jobs is from the Node.js implementation assumption.
            # We should verify if this endpoint supports this rich configuration or if we need another one.
            # For now, we follow the pattern.
            response = client.post(f"{NEMO_API_URL}/v1/jobs", json=payload, timeout=10.0)
            response.raise_for_status()
            data = response.json()
            return f"Job submitted successfully! ID: {data.get('job_id', 'unknown')}"
    except Exception as e:
        return f"Failed to submit job: {str(e)}"

if __name__ == "__main__":
    mcp.run()
